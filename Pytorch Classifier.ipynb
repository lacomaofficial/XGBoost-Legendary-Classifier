{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch_directml\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "\n",
    "# Device selection: DirectML or fallback to CUDA/CPU\n",
    "device = torch_directml.device() if torch_directml.is_available() else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to the dataset\n",
    "data_url = 'https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv'\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(data_url, index_col='Name')\n",
    "df = df.drop(['#'], axis=1)\n",
    "\n",
    "# Map Legendary to binary values\n",
    "df['Legendary'] = df['Legendary'].map({False: 0, True: 1})\n",
    "\n",
    "# One-hot encode categorical variables and handle missing values\n",
    "df = pd.get_dummies(df, columns=['Type 1', 'Type 2'], dummy_na=True)\n",
    "\n",
    "# Separate features and target\n",
    "x_columns = df.drop('Legendary', axis=1).columns\n",
    "x = df[x_columns].astype(float).values\n",
    "y = df['Legendary'].values\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: privateuseone:0\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "BATCH_SIZE = 32\n",
    "TEST_SIZE = 0.25\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Split into train/test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "\n",
    "# Move data to the appropriate device (DirectML, CUDA, or CPU)\n",
    "x_train, y_train, x_test, y_test = [t.to(device) for t in [x_train, y_train, x_test, y_test]]\n",
    "\n",
    "# DataLoader creation\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advance Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-24 12:16:52,455] A new study created in memory with name: no-name-0d18d360-28d6-4d03-ad59-46d9de17b810\n",
      "c:\\Users\\lacom\\.conda\\envs\\xtra\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [256, 128] which is of type list.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\lacom\\.conda\\envs\\xtra\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [512, 256, 128] which is of type list.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\lacom\\.conda\\envs\\xtra\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [512, 256] which is of type list.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\lacom\\AppData\\Local\\Temp\\ipykernel_15140\\1518878645.py:39: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n",
      "C:\\Users\\lacom\\AppData\\Local\\Temp\\ipykernel_15140\\1518878645.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n",
      "C:\\Users\\lacom\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\functional.py:3154: UserWarning: The operator 'aten::binary_cross_entropy' is not currently supported on the DML backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at C:\\__w\\1\\s\\pytorch-directml-plugin\\torch_directml\\csrc\\dml\\dml_cpu_fallback.cpp:17.)\n",
      "  return torch._C._nn.binary_cross_entropy(input, target, weight, reduction_enum)\n",
      "[I 2024-08-24 12:18:15,483] Trial 0 finished with value: 0.09839651081711054 and parameters: {'hidden_layers': [512, 256], 'dropout_rate': 0.33210047754426975, 'lr': 0.00051556418093953, 'weight_decay': 0.0002151813490372811}. Best is trial 0 with value: 0.09839651081711054.\n",
      "c:\\Users\\lacom\\.conda\\envs\\xtra\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [256, 128] which is of type list.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\lacom\\.conda\\envs\\xtra\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [512, 256, 128] which is of type list.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\lacom\\.conda\\envs\\xtra\\Lib\\site-packages\\optuna\\distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [512, 256] which is of type list.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\lacom\\AppData\\Local\\Temp\\ipykernel_15140\\1518878645.py:39: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n",
      "C:\\Users\\lacom\\AppData\\Local\\Temp\\ipykernel_15140\\1518878645.py:40: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n",
      "[I 2024-08-24 12:24:57,763] Trial 1 finished with value: 0.16772862949541636 and parameters: {'hidden_layers': [512, 256, 128], 'dropout_rate': 0.4535601296897447, 'lr': 4.298653906155615e-05, 'weight_decay': 4.121577731662978e-05}. Best is trial 0 with value: 0.09839651081711054.\n",
      "[I 2024-08-24 12:25:53,912] Trial 2 finished with value: 0.08728813752532005 and parameters: {'hidden_layers': [512, 256, 128], 'dropout_rate': 0.4427843287948138, 'lr': 0.0024512507844197074, 'weight_decay': 0.00011099599283886396}. Best is trial 2 with value: 0.08728813752532005.\n",
      "[I 2024-08-24 12:27:01,007] Trial 3 finished with value: 0.09277860700551953 and parameters: {'hidden_layers': [512, 256], 'dropout_rate': 0.28510793514397464, 'lr': 0.0004889245201367542, 'weight_decay': 2.3152218435805585e-06}. Best is trial 2 with value: 0.08728813752532005.\n",
      "[I 2024-08-24 12:28:01,142] Trial 4 finished with value: 0.07967968860508076 and parameters: {'hidden_layers': [256, 128], 'dropout_rate': 0.29561935187464167, 'lr': 0.005535215564704708, 'weight_decay': 0.0001776278936979131}. Best is trial 4 with value: 0.07967968860508076.\n",
      "[I 2024-08-24 12:28:52,225] Trial 5 finished with value: 0.09009173472544976 and parameters: {'hidden_layers': [512, 256, 128], 'dropout_rate': 0.3403866350111805, 'lr': 0.004740386993836942, 'weight_decay': 1.8161761213287032e-05}. Best is trial 4 with value: 0.07967968860508076.\n",
      "[I 2024-08-24 12:30:37,588] Trial 6 finished with value: 0.0897686340446983 and parameters: {'hidden_layers': [512, 256], 'dropout_rate': 0.48874798809395387, 'lr': 0.0005683856218573612, 'weight_decay': 6.791911073257922e-05}. Best is trial 4 with value: 0.07967968860508076.\n",
      "[I 2024-08-24 12:32:39,566] Trial 7 finished with value: 0.10208772708262716 and parameters: {'hidden_layers': [512, 256, 128], 'dropout_rate': 0.23571033124231938, 'lr': 0.000383591233145811, 'weight_decay': 0.00023342888856778642}. Best is trial 4 with value: 0.07967968860508076.\n",
      "[I 2024-08-24 12:34:33,422] Trial 8 finished with value: 0.0889198511971959 and parameters: {'hidden_layers': [256, 128], 'dropout_rate': 0.39605924190044683, 'lr': 0.0011190960623279431, 'weight_decay': 1.369582425511e-05}. Best is trial 4 with value: 0.07967968860508076.\n",
      "[I 2024-08-24 12:36:05,609] Trial 9 finished with value: 0.08839262715939965 and parameters: {'hidden_layers': [512, 256], 'dropout_rate': 0.21276422682109308, 'lr': 0.0004967918864101518, 'weight_decay': 0.0008591720297801569}. Best is trial 4 with value: 0.07967968860508076.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_layers': [256, 128], 'dropout_rate': 0.29561935187464167, 'lr': 0.005535215564704708, 'weight_decay': 0.0001776278936979131}\n"
     ]
    }
   ],
   "source": [
    "# Model Definition\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers, dropout_rate):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "        layers = []\n",
    "        last_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_layers:\n",
    "            layers.append(nn.Linear(last_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.LeakyReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            last_dim = hidden_dim\n",
    "            \n",
    "        layers.append(nn.Linear(last_dim, 1))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Hyperparameter Optimization Function\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    hidden_layers = trial.suggest_categorical('hidden_layers', [[256, 128], [512, 256, 128], [512, 256]])\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5)\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n",
    "\n",
    "    # Model and optimizer\n",
    "    model = ClassificationModel(input_dim=x_train.shape[1], hidden_layers=hidden_layers, dropout_rate=dropout_rate).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.BCELoss()\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=MAX_EPOCHS)\n",
    "    \n",
    "    # Training loop\n",
    "    best_loss = float('inf')\n",
    "    no_improvement = 0\n",
    "    patience = 10\n",
    "    \n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = sum(criterion(model(inputs.to(device)), labels.to(device)).item() for inputs, labels in test_loader) / len(test_loader)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_loss - 1e-3:\n",
    "            best_loss = val_loss\n",
    "            no_improvement = 0\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "\n",
    "        if no_improvement >= patience:\n",
    "            break\n",
    "\n",
    "    return best_loss\n",
    "\n",
    "# Create study and optimize\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10) #n_trials recommended = 50\n",
    "\n",
    "# Best model parameters\n",
    "best_params = study.best_params\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, scheduler, max_epochs, patience):\n",
    "    best_loss = float('inf')\n",
    "    no_improvement = 0\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = sum(criterion(model(inputs.to(device)), labels.to(device)).item() for inputs, labels in test_loader) / len(test_loader)\n",
    "\n",
    "        if val_loss < best_loss - 1e-3:\n",
    "            best_loss = val_loss\n",
    "            no_improvement = 0\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "\n",
    "        if no_improvement >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(X_test).cpu().numpy().flatten()\n",
    "        pred = np.clip(pred, a_min=1e-6, a_max=1-1e-6)\n",
    "        logloss = metrics.log_loss(y_test.cpu().numpy(), pred)\n",
    "        pred_binary = (pred > 0.5).astype(int)\n",
    "        accuracy = metrics.accuracy_score(y_test.cpu().numpy(), pred_binary)\n",
    "        auc_roc = roc_auc_score(y_test.cpu().numpy(), pred)\n",
    "        precision, recall, _ = precision_recall_curve(y_test.cpu().numpy(), pred)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        return logloss, accuracy, auc_roc, pr_auc\n",
    "\n",
    "# Feature importance\n",
    "def perturbation_rank(model, x_test, y_test, feature_names, verbose=False):\n",
    "    model.eval()\n",
    "    baseline_loss = criterion(model(x_test), y_test).item()\n",
    "    importance_scores = []\n",
    "\n",
    "    for i in range(x_test.shape[1]):\n",
    "        x_test_perturbed = x_test.clone()\n",
    "        x_test_perturbed[:, i] = x_test_perturbed[torch.randperm(x_test_perturbed.size(0)), i]\n",
    "        perturbed_loss = criterion(model(x_test_perturbed), y_test).item()\n",
    "        importance = perturbed_loss - baseline_loss\n",
    "        importance_scores.append(importance)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Feature {feature_names[i]} - Perturbed Loss: {perturbed_loss:.4f} - Importance: {importance:.4f}\")\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importance_scores\n",
    "    }).sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return importance_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 14\n",
      "Validation logloss: 0.12320013152055614\n",
      "Validation accuracy score: 0.95\n",
      "Validation AUC-ROC: 0.9806663924310983\n",
      "Validation Precision-Recall AUC: 0.7879967334044045\n"
     ]
    }
   ],
   "source": [
    "# Train final model with best parameters\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "best_model = ClassificationModel(input_dim=x_train.shape[1], hidden_layers=best_params['hidden_layers'], \n",
    "                                 dropout_rate=best_params['dropout_rate']).to(device)\n",
    "best_optimizer = optim.AdamW(best_model.parameters(), lr=best_params['lr'], weight_decay=best_params['weight_decay'])\n",
    "best_scheduler = optim.lr_scheduler.CosineAnnealingLR(best_optimizer, T_max=MAX_EPOCHS)\n",
    "\n",
    "final_model = train_model(best_model, train_loader, test_loader, criterion, best_optimizer, best_scheduler, MAX_EPOCHS, PATIENCE)\n",
    "\n",
    "# Evaluate model\n",
    "logloss, accuracy, auc_roc, pr_auc = evaluate_model(final_model, x_test, y_test)\n",
    "print(f\"Validation logloss: {logloss}\")\n",
    "print(f\"Validation accuracy score: {accuracy}\")\n",
    "print(f\"Validation AUC-ROC: {auc_roc}\")\n",
    "print(f\"Validation Precision-Recall AUC: {pr_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Feature  Importance\n",
      "0          Attack    0.101388\n",
      "1         Sp. Atk    0.076471\n",
      "2           Speed    0.066158\n",
      "3         Defense    0.055552\n",
      "4         Sp. Def    0.027226\n",
      "5              HP    0.009540\n",
      "6      Generation    0.002305\n",
      "7  Type 1_Psychic    0.000918\n",
      "8   Type 1_Ground    0.000780\n",
      "9      Type 2_nan    0.000636\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance\n",
    "importance_df = perturbation_rank(final_model, x_test, y_test, feature_names)\n",
    "print(importance_df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xtra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
